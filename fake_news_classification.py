# -*- coding: utf-8 -*-
"""Fake News Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lYtya_W2HgCqiH_zViR4v_NBZAZ0ygP8

# 1. Importing Packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.model_selection import train_test_split

PKG_DIR="/content/drive/MyDrive/offline_packages"

!pip install --no-index --find-links="$PKG_DIR" transformers datasets evaluate

from transformers import AutoConfig,AutoModelForSequenceClassification, AutoTokenizer
from datasets import Dataset, DatasetDict
from transformers import TrainingArguments, Trainer
import evaluate

"""# 2. EDA"""

df = pd.read_excel("https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/master/fake_news.xlsx")
df.head()

df[['title','text']]

df.info()

print(df.isna().sum())
df.dropna(inplace=True)
print("\nDone Deleting\n")
print(df.isna().sum())

df.duplicated().sum()

df['label'].value_counts()

sns.barplot(x=df['label'].value_counts().index, y=df['label'].value_counts().values)
plt.xlabel("Label")
plt.ylabel("Count")
plt.title("Label Distribution")
plt.show()

# 1.5 tokens per word on average
df['title_tokens'] = df['title'].apply(lambda x: len(x.split())*1.5)
df['text_tokens'] = df['text'].apply(lambda x: len(x.split())*1.5)

fix,ax = plt.subplots(1,2,figsize=(15,5))

ax[0].hist(df['title_tokens'], bins=50, color = 'skyblue')
ax[0].set_title("Title Tokens")

ax[1].hist(df['text_tokens'], bins=50, color = 'orange')
ax[1].set_title("Text Tokens")

plt.show()

df.rename(columns={'label':'labels'}, inplace=True)
df.head()

"""# 3. Train-Test-Split"""

train,test = train_test_split(df, test_size=0.3, random_state=42,stratify=df['labels'])
test,validation = train_test_split(test, test_size=1/3, random_state=42,stratify=test['labels'])

print(train.shape,test.shape,validation.shape)

"""# 4. Dataframe -> HF Dataset"""

dataset = DatasetDict({
    'train': Dataset.from_pandas(train,preserve_index=False),
    'test': Dataset.from_pandas(test,preserve_index=False),
    'validation': Dataset.from_pandas(validation,preserve_index=False)
})
dataset

"""# 5. label2id, id2label"""

label2id = {"Real": 0, "Fake": 1}
id2label = {0:"Real", 1:"Fake"}

"""# 6. Model, Tokenizer"""

distilbert_dir = '/content/drive/MyDrive/offline_models/distilbert-base-uncased'
tinybert_dir = '/content/drive/MyDrive/offline_models/tinybert'

distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_dir,local_files_only=True)
distilber_model = AutoModelForSequenceClassification.from_pretrained(distilbert_dir,local_files_only=True,num_labels=2,id2label=id2label,label2id=label2id)

tinybert_tokenizer = AutoTokenizer.from_pretrained(tinybert_dir,local_files_only=True)
tinybert_model = AutoModelForSequenceClassification.from_pretrained(tinybert_dir,local_files_only=True,num_labels=2,id2label=id2label,label2id=label2id)

"""# 7. Tokenization

1.   combine title+text
2.   text only
"""

def tokenize(batch):
  return tinybert_tokenizer(batch['title'], batch['text'], truncation=True,padding=True,max_length = 512)

tokenized_dataframe = dataset.map(tokenize, batched=True, batch_size=None)
tokenized_dataframe

print(dataset['train'][0],"\n")
print(tokenize(dataset['train'][0]))

final_dataset = tokenized_dataframe.remove_columns(['title','text','id','author'])
final_dataset

"""# 8. Compute metrics"""

accuracy = evaluate.load('accuracy')

def compute_metrics(eval_pred):
  predictions,labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return accuracy.compute(predictions=predictions, references=labels)

"""# 9. Training args and trainer - TinyBERT"""

batch_size = 32
training_dir = "train_dir"

training_args = TrainingArguments(
                                  output_dir=training_dir,
                                  overwrite_output_dir = True,
                                  eval_strategy = 'epoch',
                                  num_train_epochs = 3,
                                  learning_rate = 2e-5,
                                  per_device_train_batch_size = batch_size,
                                  per_device_eval_batch_size = batch_size,
                                  weight_decay = 0.01,
)

trainer = Trainer(
    model = tinybert_model,
    args = training_args,
    train_dataset = final_dataset['train'],
    eval_dataset = final_dataset['validation'],
    tokenizer = tinybert_tokenizer,
    compute_metrics = compute_metrics,
)

"""# 10. Model Training"""

trainer.train()

"""# 11. Model Evaluation"""

preds = trainer.predict(final_dataset['test'])
preds.metrics

from sklearn.metrics import accuracy_score

y_preds = np.argmax(preds.predictions, axis=1)
y_true = dataset['test']['labels'][:]

print(accuracy_score(y_true, y_preds))

"""# 12. Comparing with DistilBERT

"""

distilbert_dir = '/content/drive/MyDrive/offline_models/distilbert-base-uncased'

distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_dir,local_files_only=True)
distilbert_config = AutoConfig.from_pretrained(distilbert_dir,local_files_only=True,num_labels=2,id2label=id2label,label2id=label2id)
distilbert_model = AutoModelForSequenceClassification.from_pretrained(distilbert_dir,local_files_only=True,config = distilbert_config)

def distil_tokenize(batch):
  return distilbert_tokenizer(batch['title'], batch['text'], truncation=True,padding=True,max_length = 512)

dataset_distil = dataset.map(distil_tokenize, batched=True, batch_size=None)
final_dataset_distil = dataset_distil.remove_columns(['title','text','id','author'])

trainer_distil = Trainer(
    model = distilbert_model,
    args = training_args,
    train_dataset = final_dataset_distil['train'],
    eval_dataset = final_dataset_distil['validation'],
    tokenizer = distilbert_tokenizer,
    compute_metrics = compute_metrics,
)

trainer_distil.train()

preds_distil = trainer_distil.predict(final_dataset_distil['test'])
preds_distil.metrics

y_preds_distil = np.argmax(preds_distil.predictions, axis=1)
y_true_distil = dataset['test']['labels'][:]

print(accuracy_score(y_true_distil, y_preds_distil))

"""# Comparision"""

comparision_table = pd.DataFrame({
    'Models' : ['DistilBERT','TinyBERT'],
    'Test Accuracy' : [preds_distil.metrics['test_accuracy'],preds.metrics['test_accuracy']],
    'Training Time' : [1842.8176,494.3633],
    'Inference Time' : [preds_distil.metrics['test_runtime'],preds.metrics['test_runtime']]
})

comparision_table

